{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToPILImage\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "from flmr import index_custom_collection\n",
    "from flmr import FLMRQueryEncoderTokenizer, FLMRContextEncoderTokenizer, FLMRModelForRetrieval\n",
    "\n",
    "# load models\n",
    "checkpoint_path = \"LinWeizheDragon/PreFLMR_ViT-B\"\n",
    "image_processor_name = \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\"\n",
    "\n",
    "query_tokenizer = FLMRQueryEncoderTokenizer.from_pretrained(checkpoint_path, subfolder=\"query_tokenizer\")\n",
    "context_tokenizer = FLMRContextEncoderTokenizer.from_pretrained(\n",
    "    checkpoint_path, subfolder=\"context_tokenizer\"\n",
    ")\n",
    "\n",
    "print(\"Loading\")\n",
    "model = FLMRModelForRetrieval.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    query_tokenizer=query_tokenizer,\n",
    "    context_tokenizer=context_tokenizer,\n",
    ")\n",
    "image_processor = AutoImageProcessor.from_pretrained(image_processor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = 100\n",
    "feature_dim = 1664\n",
    "passage_contents = [f\"This is test sentence {i}\" for i in range(num_items)]\n",
    "# Option 1. text-only documents\n",
    "custom_collection = passage_contents\n",
    "# Option 2. multi-modal documents with pre-extracted image features\n",
    "# passage_image_features = np.random.rand(num_items, feature_dim)\n",
    "# custom_collection = [\n",
    "#     (passage_content, passage_image_feature, None) for passage_content, passage_image_feature in zip(passage_contents, passage_image_features)\n",
    "# ]\n",
    "# Option 3. multi-modal documents with images\n",
    "# random_images = torch.randn(num_items, 3, 224, 224)\n",
    "# to_img = ToPILImage()\n",
    "# if not os.path.exists(\"./test_images\"):\n",
    "#     os.makedirs(\"./test_images\")\n",
    "# for i, image in enumerate(random_images):\n",
    "#     image = to_img(image)\n",
    "#     image.save(os.path.join(\"./test_images\", \"{}.jpg\".format(i)))\n",
    "\n",
    "# image_paths = [os.path.join(\"./test_images\", \"{}.jpg\".format(i)) for i in range(num_items)]\n",
    "\n",
    "# custom_collection = [\n",
    "#     (passage_content, None, image_path)\n",
    "#     for passage_content, image_path in zip(passage_contents, image_paths)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_custom_collection(\n",
    "    custom_collection=custom_collection,\n",
    "    model=model,\n",
    "    index_root_path=\".\",\n",
    "    index_experiment_name=\"test_experiment\",\n",
    "    index_name=\"test_index\",\n",
    "    nbits=8, # number of bits in compression\n",
    "    doc_maxlen=512, # maximum allowed document length\n",
    "    overwrite=True, # whether to overwrite existing indices\n",
    "    use_gpu=False, # whether to enable GPU indexing\n",
    "    indexing_batch_size=64,\n",
    "    model_temp_folder=\"tmp\",\n",
    "    nranks=1, # number of GPUs used in indexing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_encoder_embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
