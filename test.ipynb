{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import *\n",
    "\n",
    "\n",
    "class AttentionFusionBertModel(BertModel):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        attention_adj: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n",
    "            )\n",
    "        elif input_ids is not None:\n",
    "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = (\n",
    "            past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "        )\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(\n",
    "                ((batch_size, seq_length + past_key_values_length)), device=device\n",
    "            )\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n",
    "                    batch_size, seq_length\n",
    "                )\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(\n",
    "                    input_shape, dtype=torch.long, device=device\n",
    "                )\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n",
    "            attention_mask, input_shape\n",
    "        )\n",
    "        \n",
    "\n",
    "        \n",
    "        if attention_adj is not None:\n",
    "            if attention_adj.dim() == 3:\n",
    "                attention_adj = attention_adj[:, None, :, :]\n",
    "\n",
    "            if attention_adj.shape[-1] != extended_attention_mask.shape[-1] or len(attention_adj.shape) != len(extended_attention_mask.shape):\n",
    "                raise ValueError(\n",
    "                    f\"Shape of attention_adj does not match extended_attention_mask \"\n",
    "                )\n",
    "            if attention_adj.device != extended_attention_mask.device:\n",
    "                raise ValueError(\n",
    "                    \"attention_adj and extended_attention_mask must be on the same device\"\n",
    "                )\n",
    "            if attention_adj.dtype != extended_attention_mask.dtype:\n",
    "                raise ValueError(\n",
    "                    \"attention_adj and extended_attention_mask must have the same data type\"\n",
    "                )\n",
    "            print(extended_attention_mask.shape)\n",
    "            print(attention_adj.shape)\n",
    "                \n",
    "            extended_attention_mask = extended_attention_mask + attention_adj\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = (\n",
    "                encoder_hidden_states.size()\n",
    "            )\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(\n",
    "                encoder_attention_mask\n",
    "            )\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = (\n",
    "            self.pooler(sequence_output) if self.pooler is not None else None\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionFusionBertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 32, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 32)\n",
      "    (token_type_embeddings): Embedding(2, 32)\n",
      "    (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=128, out_features=32, bias=True)\n",
      "          (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "# Define a basic configuration with minimal settings\n",
    "config = BertConfig(\n",
    "    # The number of hidden layers (transformer blocks)\n",
    "    num_hidden_layers=1,\n",
    "    # The hidden size of the encoder and pooler layers\n",
    "    hidden_size=32,\n",
    "    # The number of attention heads in each attention layer\n",
    "    num_attention_heads=4,\n",
    "    # The size of the input vocab\n",
    "    vocab_size=30522,  # This is the vocab size for BERT base; adjust if using a different model\n",
    "    # The maximum length of the input sequences\n",
    "    max_position_embeddings=512,\n",
    "    # The size of the intermediate (feed-forward) layer in the transformer\n",
    "    intermediate_size=128,\n",
    "    # Dropout probability\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    # Specify whether to add a pooling layer on top of the attention output for classification tasks\n",
    "    add_pooling_layer=True\n",
    ")\n",
    "\n",
    "# Initialize the BertModel with the specified configuration\n",
    "model = AttentionFusionBertModel(config)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1, 10])\n",
      "torch.Size([2, 1, 10, 10])\n",
      "torch.Size([2, 1, 10, 10])\n",
      "torch.Size([2, 4, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the sequence length for your dummy inputs\n",
    "seq_length = 10\n",
    "\n",
    "# Create random embeddings as dummy input data\n",
    "# (batch size, sequence length, hidden size)\n",
    "inputs_embeds = torch.rand((2, seq_length, config.hidden_size))\n",
    "\n",
    "# Create an attention mask (batch size, sequence length)\n",
    "# Here assuming no padding, so mask is all ones\n",
    "attention_mask = torch.ones((2, seq_length))\n",
    "\n",
    "attention_adj = torch.ones((2, seq_length, seq_length)) * -0.8\n",
    "\n",
    "# You can also simulate having some padding by making part of the mask zeros\n",
    "# For example, if only the first 7 tokens are not padding:\n",
    "# attention_mask[:, 7:] = 0\n",
    "\n",
    "# Run the model with the embedded input\n",
    "outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, return_dict=True, attention_adj=attention_adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4028234663852886e+38"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.finfo(torch.float32).min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
