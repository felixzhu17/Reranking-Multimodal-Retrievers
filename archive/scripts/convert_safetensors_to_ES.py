import torch
from safetensors import safe_open
from safetensors.torch import save_file
import pickle
import glob
from tqdm import tqdm
from datasets import Dataset
from pprint import pprint

# read all pickle files under the directory
def read_pickle_files(path):
    files = glob.glob(path + "/*.safetensors")
    return files

all_files = read_pickle_files("/home/wl356/big_picture_rds/wl356/infoseek/extracted_image_features_safetensors")

print(all_files[:3])
# all_files = all_files[:100]

# construct a dataset from all_files
ds = Dataset.from_dict({"file": all_files})


# Prepare ElasticSearch
from elasticsearch import Elasticsearch, helpers

# Password for the 'elastic' user generated by Elasticsearch
ELASTIC_PASSWORD = "EKJ8kIbmMg=e6sI5vikP"

es = Elasticsearch(
    "https://localhost:9200",
    ca_certs="/home/wl356/cvnlp_rds/wl356/elasticsearch-8.7.0/config/certs/http_ca.crt",
    basic_auth=("elastic", ELASTIC_PASSWORD)
)

# Successful response!
es.info()

index_name = "encoded_image_features"

# delete the current index
# es.indices.delete(index=index_name)


def process_examples(examples):
    all_files = examples["file"]

    actions = []

    # read pickle files and save as safe tensors
    for file in all_files:
        with safe_open(file, framework="pt") as f:
            feature = f.get_tensor('data')
        
        feature = feature.tolist()

        file_id = file.split("/")[-1].replace(".safetensors", "")
        
        action = {
            '_op_type': "index",
            '_index': index_name,
            '_id': file_id,
            '_source': {
                'feature': feature,
            }
        }
        actions.append(action)
    
    
    res = helpers.bulk(es, actions, request_timeout=120)
    # print(f"number of success {res[0]}")
    if res[0] != len(actions):
        print("errors", res[1])
    print(f"Successfully indexed {len(actions)} items into ES.")


ds.map(process_examples, batched=True, batch_size=1000, num_proc=16)


queries = [
    'oven_00096930.jpg|||building_0.0_135.49_438.22_598.0',
    'oven_04539982.JPEG|||flower_1.21_91.95_31.17_109.54',
]
docs = [
    {
        '_index': index_name,
        '_id': q,
    } for q in queries
]
resp = es.mget(index=index_name, docs=docs)
for doc in resp['docs']:
    if doc['found']:
        print(doc['_id'], torch.FloatTensor(doc['_source']['feature']))
    else:
        print(doc['_id'], 'not found')
