from runway_for_ml.data_module.data_transforms import BaseTransform, HFDatasetTransform, register_transform_functor, keep_ds_columns

import os
import re
import sys
import time
import json
import copy
from tqdm import tqdm
import csv
import json
import torch
import pickle
import numpy as np
import pandas as pd
import scipy.sparse as sp
import random
import cv2
import base64
from PIL import Image

from copy import deepcopy
from pprint import pprint
from easydict import EasyDict
from collections import defaultdict
from datasets import load_dataset, Dataset, DatasetDict
from datasets import concatenate_datasets
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

import logging
logger = logging.getLogger(__name__)

from utils.dirs import create_dirs
import numpy as np

from transformers import AutoImageProcessor
from flmr import FLMRQueryEncoderTokenizer, FLMRContextEncoderTokenizer, FLMRModelForRetrieval


@register_transform_functor
class PrepareDistillationScores(HFDatasetTransform):
    """
    This functor prepare negative examples and their scores using FLMR
    """
    def setup(self, 
              model_ckpt, 
              image_processor_name, 
              splits_to_process=["train"], 
              num_negatives=4, 
              limit_num_data=None,
              _num_gpu_proc=None,
              _use_elastic_search=True,
              _index_name="distillation_data",
              _source_name="default",
              *args, **kwargs):
        self.config = self.global_config
        self.model_ckpt = model_ckpt
        self.image_processor_name = image_processor_name
        self.splits_to_process = splits_to_process
        self.num_negatives = num_negatives
        self.limit_num_data = limit_num_data
        self.use_elastic_search = _use_elastic_search
        self.index_name = _index_name + "_" + _source_name
        self.source_name = _source_name
        self.num_gpu_proc = _num_gpu_proc


    def _call(self, data,  *args, **kwargs):
        res = DatasetDict()

        checkpoint_path = self.model_ckpt

        query_tokenizer = FLMRQueryEncoderTokenizer.from_pretrained(checkpoint_path, subfolder="query_tokenizer")
        context_tokenizer = FLMRContextEncoderTokenizer.from_pretrained(checkpoint_path, subfolder="context_tokenizer")
        
        model = FLMRModelForRetrieval.from_pretrained(checkpoint_path,
                                                        query_tokenizer=query_tokenizer,
                                                        context_tokenizer=context_tokenizer,
                                                        transformer_mapping_config_base="/root/rds/data/models/bert-base-uncased", # Temp code to avoid errors. TODO: remove
                                                        )
        image_processor = AutoImageProcessor.from_pretrained(self.image_processor_name)
        model.eval()

        # model = model.cuda()
        
        if self.use_elastic_search:
            index_name = self.index_name

            # Prepare ElasticSearch
            from elasticsearch import Elasticsearch, helpers

            # Password for the 'elastic' user generated by Elasticsearch
            ELASTIC_PASSWORD = os.environ["ELASTIC_PASSWORD"]

            es = Elasticsearch(
                "https://localhost:9200",
                ca_certs=os.environ["ELASTIC_CA_CERTS"],
                basic_auth=("elastic", ELASTIC_PASSWORD)
            )

            if not es.indices.exists(index=index_name):
                es.indices.create(index=index_name)
        

        for split in self.splits_to_process:
            split_data = data[split]
            split_passages = data[f"{split}_passages"]
            print(split_passages[0])
            print(split_data[0])
            print(f"Before removing images that do not exist: {len(split_data)}")
            def check_file_exist_and_can_open(example):
                if example['img_path'] is not None:
                    if os.path.exists(example['img_path']):
                        # try:
                        #     Image.open(example['img_path'])
                        # except Exception as e:
                        #     print(e)
                        #     return False
                        return True
                    else:
                        return False
                return True
            split_data = split_data.filter(
                lambda example: check_file_exist_and_can_open(example), 
                num_proc=16, 
                load_from_cache_file=False
            )
            print(f"After removing images that do not exist: {len(split_data)}")

            def read_cached_data_from_es(batch):
                if not self.use_elastic_search:
                    batch['neg_item_indices'] = [None] * len(batch['question_id'])
                    batch['neg_item_ids'] = [None] * len(batch['question_id'])
                    batch['neg_item_contents'] = [None] * len(batch['question_id'])
                    batch['scores'] = [None] * len(batch['question_id'])
                    batch['found'] = [False] * len(batch['question_id'])
                    return batch
                
                es = Elasticsearch(
                    "https://localhost:9200",
                    ca_certs=os.environ["ELASTIC_CA_CERTS"],
                    basic_auth=("elastic", ELASTIC_PASSWORD),
                    timeout=60,
                )
                batch_question_ids = batch['question_id']
                batch_ids_to_search = [f"{self.source_name}_{question_id}" for question_id in batch_question_ids]

                found = []
                batch_neg_item_indices = []
                batch_neg_item_ids = []
                batch_neg_item_contents = []
                batch_scores = []

                queries = batch_ids_to_search
                docs = [
                    {
                        '_index': index_name,
                        '_id': q,
                    } for q in queries
                ]
                resp = es.mget(index=index_name, docs=docs)
                for doc in resp['docs']:
                    if doc['found']:
                        batch_neg_item_indices.append(doc['_source']['neg_item_indices'])
                        batch_neg_item_ids.append([split_passages[i]['passage_id'] for i in doc['_source']['neg_item_indices']])
                        batch_neg_item_contents.append([split_passages[i]['passage_content'] for i in doc['_source']['neg_item_indices']])
                        batch_scores.append(doc['_source']['score'])
                        found.append(True)
                    else:
                        batch_neg_item_indices.append(None)
                        batch_neg_item_ids.append(None)
                        batch_neg_item_contents.append(None)
                        batch_scores.append(None)
                        found.append(False)

                # report number of found
                # print(f"found {sum(found)} out of {len(found)}")
                
                batch['neg_item_indices'] = batch_neg_item_indices
                batch['neg_item_ids'] = batch_neg_item_ids
                batch['neg_item_contents'] = batch_neg_item_contents
                batch['scores'] = batch_scores
                batch['found'] = found

                return batch

            def obtain_negatives(example):
                pos_item_ids = example['pos_item_ids']
                # randomly select self.num_negatives documents from split_passages
                num_total = len(split_passages)
                neg_item_ids = []
                neg_item_indices = []
                neg_item_contents = []
                while len(neg_item_ids) < self.num_negatives:
                    neg_item_index = random.randint(0, num_total-1)
                    neg_item_id = split_passages[neg_item_index]['passage_id']
                    if neg_item_id not in pos_item_ids:
                        neg_item_ids.append(neg_item_id)
                        neg_item_indices.append(neg_item_index)
                        neg_item_contents.append(split_passages[neg_item_index]['passage_content'])

                example['neg_item_ids'] = neg_item_ids
                example['neg_item_indices'] = neg_item_indices
                example['neg_item_contents'] = neg_item_contents

                return example

            
            def obtain_scores(examples, rank, model, image_processor):
                # Password for the 'elastic' user generated by Elasticsearch
                ELASTIC_PASSWORD = os.environ["ELASTIC_PASSWORD"]

                es = Elasticsearch(
                    "https://localhost:9200",
                    ca_certs=os.environ["ELASTIC_CA_CERTS"],
                    basic_auth=("elastic", ELASTIC_PASSWORD)
                )

                if rank is not None:
                    model = model.to(torch.device(f"cuda:{rank}"))
                else:
                    rank = 0

                batch_question_ids = examples['question_id']
                batch_pos_item_contents = examples["pos_item_contents"]
                batch_neg_item_ids = examples['neg_item_ids']
                batch_neg_item_indices = examples['neg_item_indices']
                batch_neg_item_contents = [
                    [split_passages[neg_item_index]['passage_content'] for neg_item_index in neg_item_indices]
                    for neg_item_indices in batch_neg_item_indices
                ]
                batch_item_contents = [
                    [pos_item_contents[0]] + neg_item_contents for pos_item_contents, neg_item_contents in zip(batch_pos_item_contents, batch_neg_item_contents)
                ]
                concat_item_contents = [item for sublist in batch_item_contents for item in sublist]
                
                with torch.no_grad():
                    Q_encoding = query_tokenizer(examples['question'])
                    D_encoding = context_tokenizer(concat_item_contents)
                    img_paths = examples['img_path']
                    if img_paths[0] is None:
                        # This is text only
                        # Create a black image
                        images = [Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8)) for img_path in img_paths]
                    else:
                        images = [Image.open(img_path) for img_path in img_paths]
                    
                    Q_pixel_values = image_processor(images, return_tensors="pt")['pixel_values']
                    # print(Q_encoding['input_ids'].shape)
                    # print(Q_pixel_values.shape)
                    # print(D_encoding['input_ids'].shape)
                    inputs = dict(
                        query_input_ids=Q_encoding['input_ids'].to(torch.device(f"cuda:{rank}")),
                        query_attention_mask=Q_encoding['attention_mask'].to(torch.device(f"cuda:{rank}")),
                        query_pixel_values=Q_pixel_values.to(torch.device(f"cuda:{rank}")),
                        context_input_ids=D_encoding['input_ids'].to(torch.device(f"cuda:{rank}")),
                        context_attention_mask=D_encoding['attention_mask'].to(torch.device(f"cuda:{rank}")),
                        use_in_batch_negatives=True,
                        num_negative_examples=self.num_negatives,
                    )
                    scores = model.forward(**inputs).scores
                    examples['scores'] = scores.cpu().numpy().tolist()

                if self.use_elastic_search:
                    all_actions = []
                    for question_id, neg_item_indices, score_list in zip(batch_question_ids, batch_neg_item_indices, examples['scores']):
                        action = {
                            '_op_type': "index",
                            '_index': index_name,
                            '_id': f"{self.source_name}_{question_id}",
                            '_source': {
                                'neg_item_indices': neg_item_indices,
                                'score': score_list,
                            }
                        }
                        all_actions.append(action)
                    
                    batch_size = 10000
                    n_actions = len(all_actions)
                    for i in range(0, n_actions, batch_size):
                        # print(f"processing...{i}-{i+batch_size}/{n_actions}")
                        actions = all_actions[i:min(i+batch_size, n_actions)]
                        
                        res = helpers.bulk(es, actions, request_timeout=120)
                        # print(f"number of success {res[0]}")
                        # if res[0] != batch_size:
                        #     print("errors", res[1])
                    print(f"[Rank {rank}] Successfully indexed {len(actions)} items into ES.")

                return examples


            
            if self.limit_num_data is not None:
                # Randomly select data
                if len(split_data) > self.limit_num_data:
                    print(f"Limiting number of data to {self.limit_num_data} for split {split}...")
                    split_data = split_data.select(range(self.limit_num_data))
            

            split_data = split_data.map(read_cached_data_from_es, batched=True, load_from_cache_file=False, new_fingerprint="avoid_caching")
            already_cached_data = split_data.filter(lambda example: example['found'], num_proc=16, load_from_cache_file=False)
            print(f"Already cached data: {len(already_cached_data)}")
            remaining_data = split_data.filter(lambda example: not example['found'], num_proc=16, load_from_cache_file=False)
            print(f"Remaining data: {len(remaining_data)}")

            if len(remaining_data) > 0:
                print(f"Obtaining negatives for {len(remaining_data)} remaining data...")
                remaining_data = remaining_data.map(obtain_negatives, num_proc=32, load_from_cache_file=False)

                if self.num_gpu_proc is not None:
                    from multiprocess import set_start_method
                    try:
                        set_start_method('spawn', force=True)
                    except RuntimeError as e:
                        pass
                    remaining_data = remaining_data.map(
                        obtain_scores, 
                        batched=True, 
                        batch_size=64,
                        with_rank=True,
                        num_proc=self.num_gpu_proc,
                        load_from_cache_file=False,
                        fn_kwargs={
                            "model": model,
                            "image_processor": image_processor,
                        },
                    )
                else:
                    remaining_data = remaining_data.map(
                        obtain_scores, 
                        batched=True, 
                        batch_size=64, 
                        load_from_cache_file=False,
                        fn_kwargs={
                            "rank": 0,
                            "model": model,
                            "image_processor": image_processor,
                        },
                    )
                from datasets import concatenate_datasets
                split_data = concatenate_datasets([already_cached_data, remaining_data])
                print(f"dataset concatenated: {len(split_data)}")
            else:
                print("No remaining data to process.")
            

            print(split_data[0])

            data[split] = split_data
        
        return data